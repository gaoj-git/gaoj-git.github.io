---
layout: mypost
title: 线上netty服务客户端一直在连接服务端问题说明
categories: [netty]
---


## 现象
----------
线上部署了一个netty服务作为服务端，和1000个客户端保持连接，但是比较奇怪的现象是，每次线上其它服务发版本后这个服务就会出现大量的客户端掉线重新一直这样循环的现象，日志也查看了，每次客户端登陆后，服务端都是有回复登录成功的，正常情况流程就结束了，客户端就登录成功了，但是实际情况是客户端还是在一直在登录。问题排查了有段时间，但是无果，由于测试环境很难真实模拟线上的环境，所以排查起来也是比较困难。

## 分析
------------------
猜测的几种情况：
1.    内存泄漏？
	   经过线上的观察，内存占用800多m，启动参数堆内存设置为2g，无效
2.    服务的连接数过高？
	   经过观察确实是平时的一倍多，大概有2000多，但是并没有超出服务器的承受范围。
3.    项目中使用的httputil，是否是因为超时时间太长，导致服务被拖死，进而影响了netty的连接？
	   对代码进行了优化，设置超时时间为2秒，线上观察，问题依旧。
4.    线上数据库量cpu/负载高？
       会阻塞业务流程，有影响但不是根本原因，会阻塞业务线程执行。
5.    网络带宽问题？
	   线上是10M,阿里云监控平台观察没有出现异常。
	   
以上几种情况，经过线上的验证，均排除，可能会有影响，但不是根本原因
后来又去查看了下netty相关的资料，发现了端倪，**netty服务在启动的时候会创建2个线程组，一个线程组负责客户端的连接，另一个负责客户端的消息读取、发送，默认是cpu*2个数量的线程，线上服务器是4核，所以是有8个线程去处理客户端的读写，每个线程平均负责125个channel,并且每个线程处理channel也是串行执行的，所以在处理channel的读写时，自己的业务代码尽量避免长时间的阻塞，否则会影响其它channel的处理时间**，根据日志中的记录显示，一个客户端会在同一时间有3条登录请求，查阅了客户端厂商提供的开发文档，发现客户端请求登录后30秒内未收到服务端的响应，则会重发，结合netty的工作机制，猜测应该是发生了消息积压，(针对同一个channel),所以日志中才会出现同一个客户端在同一时间有多次的登录的记录

## 验证解决
------------
经过以上分析，原因和现象都对得上了，那么需要去验证下，首先在测试环境模拟了1000个netty客户端，然后连接netty服务（这里的工作线程组设置为8个，因为线上也是8个），模拟向服务端发送登录报文，然后在业务代码里模拟阻塞2秒（线上的阻塞原因是在业务代码中有向一个web服务发送http请求，每次web服务重启，就会导致这边执行阻塞2秒，http连接池配置了2秒），netty客户端则记录发送登录报文以及收到服务端回复的时间，找出最慢的10条，可以看到有的channel等待了90多秒才会收到服务端的回复，如果在线上环境中，客户端已经又向服务端发送了好几次登录报文了，导致channel的消息积压，陷入死循环，这和日志中看到的记录也对得上

| channelId    | 发送时间     | 收到回复时间 | 时间差（毫秒） |
| ------------ | ------------ | ------------ | -------------- |
cd52b68f|	2023-08-18 13:11:33.848|2023-08-18 13:13:10.026	|	96178
46a598bf|	2023-08-18 13:11:33.537|2023-08-18 13:13:09.088	|	95551
a78a7fb7|	2023-08-18 13:11:33.259|2023-08-18 13:13:08.215	|	94956
7e175def|	2023-08-18 13:11:32.957|2023-08-18 13:13:07.404	|	94447
8a1dc328|	2023-08-18 13:11:33.848|2023-08-18 13:13:06.295	|	92447
a7560ad2|	2023-08-18 13:11:33.537|2023-08-18 13:13:05.480	|	91943
95aa0027|	2023-08-18 13:11:33.259|2023-08-18 13:13:04.760	|	91501
e068534b|	2023-08-18 13:11:32.957|2023-08-18 13:13:03.976	|	91019
6eb8ef1a|	2023-08-18 13:11:33.917|2023-08-18 13:12:48.405	|	74488
5f8b3ad3|	2023-08-18 13:11:33.687|2023-08-18 13:12:47.907	|	74220

然后试着将工作线程组数量调大,如下
~~~java
private final EventLoopGroup workerGroup = new NioEventLoopGroup(50);
~~~

再次模拟，观察响应时间，依旧是找出最慢的10条

| channelId    | 发送时间     | 收到回复时间 | 时间差（毫秒） |
| ------------ | ------------ | ------------ | -------------- |
12b30b89|	2023-08-22 10:38:36.742|	2023-08-22 10:38:39.068	|	2326
a53b933a|	2023-08-22 10:38:36.743|	2023-08-22 10:38:39.023	|	2280
17bfdef5|	2023-08-22 10:38:36.761|	2023-08-22 10:38:39.023	|	2262
d7b256b4|	2023-08-22 10:38:36.763|	2023-08-22 10:38:39.023	|	2260
cbf0a72d|	2023-08-22 10:38:36.815|	2023-08-22 10:38:39.067	|	2252
c3d9a58e|	2023-08-22 10:38:36.815|	2023-08-22 10:38:39.023	|	2208
53f85466|	2023-08-22 10:38:36.876|	2023-08-22 10:38:39.067	|	2191
ec4e0bbf|	2023-08-22 10:38:36.876|	2023-08-22 10:38:39.022	|	2146
53146652|	2023-08-22 10:38:39.612|	2023-08-22 10:38:41.404	|	1792
00b95d54|	2023-08-22 10:38:39.102|	2023-08-22 10:38:40.884	|	1782

可以看到最慢的响应时间只有2秒多了，于是问题找到了，将线上的工作线程组数量设置为50个，发布上线，观察问题解决，重启web服务后，也没有出现批量掉线重连的情况了。

## 总结
其实是有两种解决方式：<br>
1.如上的设置工作线程数量调大<br>
2.将业务逻辑采用异步处理，不能阻塞I/O线程的执行<br>

netty的工作线程数量默认是cpu*2个，在工作线程读取消息并处理的时候，尽量避免存在耗时执行的逻辑，避免阻塞I/O线程的执行，如果必须有耗时执行，可以放到异步中或者增加工作线程的数量，使每个工作线程负责的客户端数量变少，降低每个channel处理的等待时间，避免触发客户端的超时机制，引起恶性循环。


	   